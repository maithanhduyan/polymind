// Generated by Copilot

import { Service } from './base-service.js';
import { Tool } from '@modelcontextprotocol/sdk/types.js';
import { z } from 'zod';

// Schema definitions for AI chunking operations
const smartChunkTextSchema = z.object({
    text: z.string().describe('Text to be intelligently chunked'),
    chunkSize: z.number().optional().describe('Target chunk size in characters (default: 1000)'),
    overlap: z.number().optional().describe('Overlap between chunks in characters (default: 100)'),
    language: z.string().optional().describe('Language code (vi for Vietnamese, en for English, default: auto-detect)'),
    preserveStructure: z.boolean().optional().describe('Preserve document structure like headers, paragraphs (default: true)'),
    documentType: z.string().optional().describe('Type of document: legal, technical, general (default: general)')
});

const analyzeDocumentStructureSchema = z.object({
    text: z.string().describe('Document text to analyze'),
    language: z.string().optional().describe('Language code (vi, en, default: auto-detect)')
});

const documentIntelligenceSchema = z.object({
    text: z.string().describe('Full document text to analyze for intelligent chunking strategy'),
    maxChunkSize: z.number().optional().describe('Maximum chunk size in characters (default: 1000)'),
    language: z.string().optional().describe('Language code (vi, en, default: auto-detect)')
});

const generateSummarySchema = z.object({
    text: z.string().describe('Text to summarize'),
    maxLength: z.number().optional().describe('Maximum summary length in characters (default: 200)'),
    language: z.string().optional().describe('Language for summary (vi, en, default: auto-detect)')
});

const extractKeywordsSchema = z.object({
    text: z.string().describe('Text to extract keywords from'),
    maxKeywords: z.number().optional().describe('Maximum number of keywords (default: 10)'),
    language: z.string().optional().describe('Language code (vi, en, default: auto-detect)')
});

interface ChunkResult {
    chunks: Array<{
        id: string;
        content: string;
        startIndex: number;
        endIndex: number;
        metadata: {
            chunkSize: number;
            keywords?: string[];
            summary?: string;
            structure?: string;
            importance?: 'high' | 'medium' | 'low';
        };
    }>;
    totalChunks: number;
    averageChunkSize: number;
    metadata: {
        originalLength: number;
        language: string;
        documentType: string;
        processingTime: number;
        intelligence?: {
            confidence: number;
            chunkingMethod: string;
            recommendedChunkSize: number;
        };
    };
}

interface DocumentStructure {
    language: string;
    documentType: string;
    sections: Array<{
        type: 'header' | 'paragraph' | 'list' | 'table' | 'code';
        content: string;
        level?: number;
        startIndex: number;
        endIndex: number;
    }>;
    metadata: {
        totalSections: number;
        estimatedReadingTime: number;
        complexity: 'simple' | 'medium' | 'complex';
    };
}

interface DocumentIntelligence {
    documentType: 'legal' | 'book' | 'technical' | 'news' | 'academic' | 'contract' | 'financial' | 'manual' | 'unknown';
    confidence: number;
    structure: {
        hasChapters: boolean;
        hasSections: boolean;
        hasNumberedItems: boolean;
        hasHeaders: boolean;
        averageParagraphLength: number;
        totalParagraphs: number;
    };
    chunkingStrategy: {
        recommendedChunkSize: number;
        chunkingMethod: 'semantic' | 'structural' | 'hybrid';
        preserveStructure: boolean;
        boundaries: Array<'sentence' | 'paragraph' | 'section' | 'chapter'>;
    };
    contentAnalysis: {
        density: 'low' | 'medium' | 'high';
        technicalTerms: number;
        legalTerms: number;
        complexity: 'simple' | 'medium' | 'complex';
    };
    language: string;
    estimatedProcessingTime: number;
}

interface SummaryResult {
    summary: string;
    originalLength: number;
    summaryLength: number;
    compressionRatio: number;
    language: string;
}

interface KeywordsResult {
    keywords: Array<{
        keyword: string;
        relevance: number;
        frequency: number;
    }>;
    totalKeywords: number;
    language: string;
}

export class AiChunkService implements Service {
    private readonly togetherApiKey: string;
    private readonly baseUrl = 'https://api.together.xyz/v1/chat/completions';
    // private readonly model = 'deepseek-ai/DeepSeek-V3';
    private readonly model = 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free';

    readonly namespace = 'ai_chunk';
    readonly name = 'AI Chunk Service';
    readonly description = 'Intelligent document chunking using AI for Vietnamese and multilingual text processing';
    readonly version = '1.0.0';    // System prompts for different AI tasks
    private readonly SYSTEM_PROMPTS = {
        DOCUMENT_INTELLIGENCE: `You are an expert document analyzer specializing in Vietnamese and multilingual text analysis. Your task is to analyze documents and recommend optimal chunking strategies.

Key responsibilities:
- Detect document type (legal, book, technical, news, academic, contract, financial, manual)
- Analyze document structure and hierarchy
- Recommend optimal chunking strategy based on document type
- Estimate processing complexity and time
- Identify semantic boundaries and natural breakpoints

Document Type Patterns:
- LEGAL: "ƒêi·ªÅu", "Kho·∫£n", "M·ª•c", laws, regulations, penalties
- BOOK: Chapters, sections, narrative flow, paragraphs
- TECHNICAL: Step-by-step instructions, specifications, diagrams
- NEWS: Headlines, lead paragraphs, quotes, datelines
- ACADEMIC: Abstract, introduction, methodology, results, conclusion
- CONTRACT: Clauses, terms, conditions, signatures
- FINANCIAL: Tables, numbers, financial statements, reports
- MANUAL: Instructions, procedures, troubleshooting

Chunking Strategies by Type:
- Legal: Preserve articles (ƒêi·ªÅu), clauses (Kho·∫£n), maintain legal context
- Book: Chapter/section boundaries, paragraph coherence, narrative flow
- Technical: Procedure steps, logical sequences, maintain instruction clarity
- News: Story units, paragraph breaks, maintain context
- Academic: Section boundaries, argument flow, citation context
- Contract: Clause boundaries, term definitions, legal obligations
- Financial: Table rows, statement sections, numerical contexts

Return detailed analysis in JSON format with confidence scores and recommendations.`,

        STRUCTURE_ANALYSIS: `You are an expert document structure analyzer. Your task is to identify and classify different sections of documents, especially Vietnamese legal and accounting documents. 

Key responsibilities:
- Identify headers, paragraphs, lists, tables, and legal articles
- Classify importance levels (high, medium, low)
- Detect section types (law article, regulation, definition, procedure)
- Preserve semantic boundaries and logical structure
- Generate accurate metadata for each section

For Vietnamese legal documents, pay attention to:
- "ƒêi·ªÅu" (Articles), "Kho·∫£n" (Clauses), "M·ª•c" (Items)
- Legal definitions and prohibited acts
- Accounting standards and regulations
- Technical terms and their contexts

Always return valid JSON format with detailed metadata.`,

        SUMMARIZATION: `You are an expert text summarizer specializing in Vietnamese legal and accounting documents. Your task is to create concise, accurate summaries while preserving key information.

Key responsibilities:
- Maintain legal accuracy and context
- Preserve important terms and definitions
- Keep regulatory requirements intact
- Generate meaningful summaries for search and reference
- Handle Vietnamese legal terminology correctly

For Vietnamese content:
- Preserve legal terms in Vietnamese
- Maintain formal legal language tone
- Include key article numbers and references
- Highlight compliance requirements and penalties

Generate summaries that are informative yet concise.`,

        KEYWORD_EXTRACTION: `You are an expert keyword extractor specializing in Vietnamese legal and accounting terminology. Your task is to identify the most relevant and important keywords for semantic search and document classification.

Key responsibilities:
- Extract domain-specific terminology (legal, accounting, tax)
- Identify Vietnamese legal concepts and terms
- Include both Vietnamese and technical terms
- Rank keywords by relevance and frequency
- Generate metadata for enhanced searchability

For Vietnamese legal/accounting documents, focus on:
- Legal concepts: "nghƒ©a v·ª•", "quy·ªÅn l·ª£i", "vi ph·∫°m", "x·ª≠ ph·∫°t"
- Accounting terms: "t√†i s·∫£n", "c√¥ng n·ª£", "doanh thu", "chi ph√≠"
- Regulatory terms: "quy ƒë·ªãnh", "ti√™u chu·∫©n", "h∆∞·ªõng d·∫´n"
- Technical standards: "VAS", "IFRS", "k·∫ø to√°n"

Return keywords with relevance scores and context information.`,

        CHUNKING_STRATEGY: `You are an expert document chunking strategist. Your task is to determine optimal semantic boundaries for text chunks to preserve meaning and context.

Key responsibilities:
- Identify natural breakpoints in documents
- Preserve logical sections and paragraphs
- Maintain context within chunks
- Consider document type and structure
- Generate chunks suitable for semantic search

For Vietnamese legal documents:
- Keep legal articles (ƒêi·ªÅu) intact when possible
- Preserve clause relationships
- Maintain definition contexts
- Keep related regulations together
- Consider cross-references and dependencies

Return chunking positions that preserve semantic integrity.`
    };

    constructor() {
        this.togetherApiKey = process.env.TOGETHER_API_KEY || '';
        if (!this.togetherApiKey) {
            console.warn('‚ö†Ô∏è TOGETHER_API_KEY not found in environment variables');
        }
    } async listTools(): Promise<{ tools: Tool[] }> {
        return {
            tools: [
                {
                    name: `${this.namespace}_document_intelligence`,
                    description: 'Analyze document type and recommend optimal chunking strategy based on content structure',
                    inputSchema: {
                        type: 'object',
                        properties: documentIntelligenceSchema.shape,
                        required: ['text'],
                        additionalProperties: false
                    }
                },
                {
                    name: `${this.namespace}_smart_chunk`,
                    description: 'Intelligently chunk text using AI analysis for optimal semantic boundaries',
                    inputSchema: {
                        type: 'object',
                        properties: smartChunkTextSchema.shape,
                        required: ['text'],
                        additionalProperties: false
                    }
                },
                {
                    name: `${this.namespace}_analyze_structure`,
                    description: 'Analyze document structure and identify sections, headers, and content types',
                    inputSchema: {
                        type: 'object',
                        properties: analyzeDocumentStructureSchema.shape,
                        required: ['text'],
                        additionalProperties: false
                    }
                },
                {
                    name: `${this.namespace}_generate_summary`,
                    description: 'Generate intelligent summary for text chunks',
                    inputSchema: {
                        type: 'object',
                        properties: generateSummarySchema.shape,
                        required: ['text'],
                        additionalProperties: false
                    }
                },
                {
                    name: `${this.namespace}_extract_keywords`,
                    description: 'Extract relevant keywords with AI analysis',
                    inputSchema: {
                        type: 'object',
                        properties: extractKeywordsSchema.shape,
                        required: ['text'],
                        additionalProperties: false
                    }
                }
            ]
        };
    }

    async callTool(name: string, args: unknown): Promise<unknown> {
        const startTime = Date.now(); try {
            switch (name) {
                case `${this.namespace}_document_intelligence`:
                    return await this.analyzeDocumentIntelligence(args as z.infer<typeof documentIntelligenceSchema>);

                case `${this.namespace}_smart_chunk`:
                    return await this.smartChunkText(args as z.infer<typeof smartChunkTextSchema>, startTime);

                case `${this.namespace}_analyze_structure`:
                    return await this.analyzeDocumentStructure(args as z.infer<typeof analyzeDocumentStructureSchema>);

                case `${this.namespace}_generate_summary`:
                    return await this.generateSummary(args as z.infer<typeof generateSummarySchema>);

                case `${this.namespace}_extract_keywords`:
                    return await this.extractKeywords(args as z.infer<typeof extractKeywordsSchema>);

                default:
                    throw new Error(`Unknown tool: ${name}`);
            }
        } catch (error) {
            console.error(`‚ùå Error in AiChunkService.callTool(${name}):`, error);
            throw error;
        }
    } private async smartChunkText(
        params: z.infer<typeof smartChunkTextSchema>,
        startTime: number
    ): Promise<ChunkResult> {
        const validated = smartChunkTextSchema.parse(params);

        const {
            text,
            chunkSize = 1000,
            overlap = 100,
            language = 'auto',
            preserveStructure = true,
            documentType = 'general'
        } = validated;

        // Step 1: Analyze document intelligence first
        console.log('üß† Analyzing document intelligence...');
        const intelligence = await this.analyzeDocumentIntelligence({
            text,
            maxChunkSize: chunkSize,
            language
        });

        console.log(`üìä Document analysis: ${intelligence.documentType} (confidence: ${intelligence.confidence})`);
        console.log(`üìè Recommended chunk size: ${intelligence.chunkingStrategy.recommendedChunkSize}`);

        // Step 2: Use recommended settings from intelligence
        const actualChunkSize = intelligence.chunkingStrategy.recommendedChunkSize;
        const detectedLanguage = intelligence.language;

        // Step 3: Analyze document structure if needed
        let structure: DocumentStructure | null = null;
        if (preserveStructure && intelligence.chunkingStrategy.preserveStructure) {
            structure = await this.analyzeDocumentStructure({ text, language: detectedLanguage });
        }

        // Step 4: Generate adaptive chunking strategy based on document type
        const chunkingStrategy = await this.generateAdaptiveChunkingStrategy(
            text,
            actualChunkSize,
            intelligence,
            structure
        );

        // Step 5: Apply chunking strategy
        const chunks = await this.applyChunkingStrategy(text, chunkingStrategy, overlap);

        // Step 6: Enhance chunks with AI metadata
        const enhancedChunks = await this.enhanceChunksWithMetadata(chunks, detectedLanguage);

        const processingTime = Date.now() - startTime;
        const averageChunkSize = enhancedChunks.reduce((sum, chunk) => sum + chunk.content.length, 0) / enhancedChunks.length;

        return {
            chunks: enhancedChunks,
            totalChunks: enhancedChunks.length,
            averageChunkSize: Math.round(averageChunkSize),
            metadata: {
                originalLength: text.length,
                language: detectedLanguage,
                documentType: intelligence.documentType,
                processingTime,
                intelligence: {
                    confidence: intelligence.confidence,
                    chunkingMethod: intelligence.chunkingStrategy.chunkingMethod,
                    recommendedChunkSize: intelligence.chunkingStrategy.recommendedChunkSize
                }
            }
        };
    } private async analyzeDocumentStructure(
        params: z.infer<typeof analyzeDocumentStructureSchema>
    ): Promise<DocumentStructure> {
        const { text, language = 'auto' } = params;

        const detectedLanguage = language === 'auto' ? await this.detectLanguage(text) : language;

        const prompt = this.buildStructureAnalysisPrompt(text, detectedLanguage);
        const aiResponse = await this.callTogetherAI(prompt, this.SYSTEM_PROMPTS.STRUCTURE_ANALYSIS);

        return this.parseStructureResponse(aiResponse, text, detectedLanguage);
    } private async generateSummary(
        params: z.infer<typeof generateSummarySchema>
    ): Promise<SummaryResult> {
        const { text, maxLength = 200, language = 'auto' } = params;

        const detectedLanguage = language === 'auto' ? await this.detectLanguage(text) : language;

        const prompt = this.buildSummaryPrompt(text, maxLength, detectedLanguage);
        const aiResponse = await this.callTogetherAI(prompt, this.SYSTEM_PROMPTS.SUMMARIZATION);        // Parse JSON response
        let summary = '';
        try {
            // Clean response and extract JSON
            let jsonText = aiResponse.trim();

            // Remove markdown code blocks
            const jsonMatch = aiResponse.match(/```(?:json)?\s*([\s\S]*?)\s*```/);
            if (jsonMatch) {
                jsonText = jsonMatch[1].trim();
            }

            // Try to extract JSON object
            const jsonStart = jsonText.indexOf('{');
            const jsonEnd = jsonText.lastIndexOf('}');
            if (jsonStart >= 0 && jsonEnd > jsonStart) {
                jsonText = jsonText.substring(jsonStart, jsonEnd + 1);
            }

            const parsed = JSON.parse(jsonText);
            summary = parsed.summary || jsonText;
        } catch (error) {
            // Fallback: use response as-is if not valid JSON
            summary = aiResponse.trim();
            // Remove markdown formatting
            summary = summary.replace(/```json\s*|\s*```/g, '');
            summary = summary.replace(/^\{.*?"summary":\s*"([^"]*)".*\}$/s, '$1');
        }

        // Remove quotes if wrapped
        if (summary.startsWith('"') && summary.endsWith('"')) {
            summary = summary.slice(1, -1);
        }

        return {
            summary,
            originalLength: text.length,
            summaryLength: summary.length,
            compressionRatio: Math.round((summary.length / text.length) * 100),
            language: detectedLanguage
        };
    }

    private async extractKeywords(
        params: z.infer<typeof extractKeywordsSchema>
    ): Promise<KeywordsResult> {
        const { text, maxKeywords = 10, language = 'auto' } = params;

        const detectedLanguage = language === 'auto' ? await this.detectLanguage(text) : language; const prompt = this.buildKeywordExtractionPrompt(text, maxKeywords, detectedLanguage);
        const aiResponse = await this.callTogetherAI(prompt, this.SYSTEM_PROMPTS.KEYWORD_EXTRACTION);

        return await this.parseKeywordsResponse(aiResponse, detectedLanguage);
    } private async callTogetherAI(prompt: string, systemPrompt?: string): Promise<string> {
        if (!this.togetherApiKey) {
            throw new Error('TOGETHER_API_KEY not configured');
        }

        const messages = [];

        // Add system prompt if provided
        if (systemPrompt) {
            messages.push({
                role: 'system',
                content: systemPrompt
            });
        }

        messages.push({
            role: 'user',
            content: prompt
        });

        console.log('üîç Calling Together AI with messages:', JSON.stringify(messages, null, 2));

        const response = await fetch(this.baseUrl, {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${this.togetherApiKey}`,
                'Content-Type': 'application/json'
            }, body: JSON.stringify({
                model: this.model,
                messages,
                max_tokens: 4000,
                temperature: 0.1,
                // Remove response_format for better compatibility
            })
        });

        if (!response.ok) {
            const errorText = await response.text();
            console.error('‚ùå Together AI API error:', response.status, response.statusText, errorText);
            throw new Error(`Together AI API error: ${response.status} ${response.statusText} - ${errorText}`);
        }

        const data = await response.json();
        console.log('‚úÖ Together AI response:', JSON.stringify(data, null, 2));
        return data.choices[0]?.message?.content || '';
    }

    private async detectLanguage(text: string): Promise<string> {
        // Simple language detection for Vietnamese
        const vietnamesePattern = /[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]/i;
        return vietnamesePattern.test(text) ? 'vi' : 'en';
    } private buildStructureAnalysisPrompt(text: string, language: string): string {
        const langInstruction = language === 'vi'
            ? 'Ph√¢n t√≠ch c·∫•u tr√∫c vƒÉn b·∫£n ti·∫øng Vi·ªát'
            : 'Analyze English text structure';

        return `${langInstruction}. Identify headers, paragraphs, lists, and document sections. 
        
Text to analyze:
${text.substring(0, 2000)}${text.length > 2000 ? '...' : ''}

IMPORTANT: Return ONLY a valid JSON object with this exact structure:
{
  "sections": [
    {
      "type": "header|paragraph|list|table|code",
      "content": "section content",
      "level": 1,
      "startIndex": 0,
      "endIndex": 100
    }
  ]
}`;
    } private buildSummaryPrompt(text: string, maxLength: number, language: string): string {
        const langInstruction = language === 'vi'
            ? `T√≥m t·∫Øt vƒÉn b·∫£n ti·∫øng Vi·ªát trong kho·∫£ng ${maxLength} k√Ω t·ª±. T√≥m t·∫Øt ph·∫£i bao g·ªìm c√°c √Ω ch√≠nh v√† gi·ªØ nguy√™n thu·∫≠t ng·ªØ quan tr·ªçng.`
            : `Summarize the English text in approximately ${maxLength} characters. Include main points and preserve important terminology.`;

        return `${langInstruction}

VƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt:
${text}

H√£y tr·∫£ v·ªÅ JSON v·ªõi format sau:
{
  "summary": "n·ªôi dung t√≥m t·∫Øt chi ti·∫øt ·ªü ƒë√¢y"
}`;
    }

    private buildKeywordExtractionPrompt(text: string, maxKeywords: number, language: string): string {
        const langInstruction = language === 'vi'
            ? `Tr√≠ch xu·∫•t ${maxKeywords} t·ª´ kh√≥a quan tr·ªçng nh·∫•t t·ª´ vƒÉn b·∫£n ti·∫øng Vi·ªát`
            : `Extract the ${maxKeywords} most important keywords from the English text`;

        return `${langInstruction}:

${text}

IMPORTANT: Return ONLY a valid JSON object with this exact structure:
{
  "keywords": [
    {
      "keyword": "keyword text",
      "relevance": 0.95,
      "frequency": 3
    }
  ]
}`;
    }

    private async generateAdaptiveChunkingStrategy(
        text: string,
        chunkSize: number,
        intelligence: DocumentIntelligence,
        structure: DocumentStructure | null
    ): Promise<Array<{ start: number; end: number; type: string }>> {

        console.log(`üéØ Using ${intelligence.chunkingStrategy.chunkingMethod} chunking for ${intelligence.documentType} document`);

        // Use different strategies based on document type
        switch (intelligence.documentType) {
            case 'legal':
                return this.generateLegalChunkingStrategy(text, chunkSize, structure);

            case 'book':
                return this.generateBookChunkingStrategy(text, chunkSize, intelligence);

            case 'technical':
                return this.generateTechnicalChunkingStrategy(text, chunkSize);

            case 'academic':
                return this.generateAcademicChunkingStrategy(text, chunkSize);

            default:
                // Fallback to enhanced semantic chunking
                return this.generateSemanticChunkingStrategy(text, chunkSize, intelligence);
        }
    }

    private generateLegalChunkingStrategy(
        text: string,
        chunkSize: number,
        structure: DocumentStructure | null
    ): Array<{ start: number; end: number; type: string }> {
        const chunks = [];

        // Look for Vietnamese legal patterns: "ƒêi·ªÅu", "Kho·∫£n", "M·ª•c"
        const articlePattern = /^(ƒêi·ªÅu \d+\..*?)(?=\n\n|ƒêi·ªÅu \d+|$)/gms;
        const clausePattern = /^(\d+\. .*?)(?=\n\d+\.|$)/gm;

        let lastEnd = 0;
        let match;

        // First try to chunk by articles (ƒêi·ªÅu)
        while ((match = articlePattern.exec(text)) !== null) {
            const start = match.index;
            let end = start + match[0].length;

            // If article is too long, split by clauses
            if (end - start > chunkSize * 1.5) {
                const articleText = match[0];
                const clauseChunks = this.splitByClauses(articleText, chunkSize, start);
                chunks.push(...clauseChunks);
            } else {
                chunks.push({
                    start,
                    end,
                    type: 'legal_article'
                });
            }
            lastEnd = end;
        }

        // Handle remaining text
        if (lastEnd < text.length) {
            chunks.push(...this.createSimpleChunks(text.substring(lastEnd), chunkSize).map(chunk => ({
                ...chunk,
                start: chunk.start + lastEnd,
                end: chunk.end + lastEnd,
                type: 'legal_text'
            })));
        }

        return chunks.length > 0 ? chunks : this.createSimpleChunks(text, chunkSize);
    }

    private generateBookChunkingStrategy(
        text: string,
        chunkSize: number,
        intelligence: DocumentIntelligence
    ): Array<{ start: number; end: number; type: string }> {
        const chunks = [];

        // Look for chapter/section patterns
        const chapterPattern = /^(ch∆∞∆°ng|chapter|ph·∫ßn)\s+\d+|^#\s+/gmi;
        const sectionPattern = /^(m·ª•c|section|\d+\.\d+)/gmi;

        // Use larger chunks for books (typically 1.5x base size)
        const bookChunkSize = Math.round(chunkSize * 1.5);

        // Split by chapters first, then by paragraphs
        const paragraphs = text.split('\n\n');
        let currentChunk = '';
        let currentStart = 0;
        let currentPos = 0;

        for (const paragraph of paragraphs) {
            const nextLength = currentChunk.length + paragraph.length + 2;

            if (nextLength > bookChunkSize && currentChunk.length > 0) {
                // Save current chunk
                chunks.push({
                    start: currentStart,
                    end: currentPos,
                    type: 'book_section'
                });

                // Start new chunk
                currentChunk = paragraph;
                currentStart = currentPos;
            } else {
                currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
            }

            currentPos += paragraph.length + 2;
        }

        // Add final chunk
        if (currentChunk.length > 0) {
            chunks.push({
                start: currentStart,
                end: text.length,
                type: 'book_section'
            });
        }

        return chunks.length > 0 ? chunks : this.createSimpleChunks(text, bookChunkSize);
    }

    private generateTechnicalChunkingStrategy(
        text: string,
        chunkSize: number
    ): Array<{ start: number; end: number; type: string }> {
        const chunks = [];

        // Look for step patterns: "B∆∞·ªõc 1", "Step 1", numbered lists
        const stepPattern = /^(b∆∞·ªõc|step|h∆∞·ªõng d·∫´n)\s+\d+/gmi;
        const numberedListPattern = /^\d+\.\s+/gm;

        // Split by steps/procedures
        const lines = text.split('\n');
        let currentChunk = '';
        let currentStart = 0;
        let currentPos = 0;

        for (let i = 0; i < lines.length; i++) {
            const line = lines[i];
            const isStep = stepPattern.test(line) || numberedListPattern.test(line);

            if (isStep && currentChunk.length > chunkSize * 0.5) {
                // Save current chunk at step boundary
                chunks.push({
                    start: currentStart,
                    end: currentPos,
                    type: 'technical_step'
                });

                currentChunk = line;
                currentStart = currentPos;
            } else {
                currentChunk += (currentChunk ? '\n' : '') + line;
            }

            currentPos += line.length + 1;
        }

        // Add final chunk
        if (currentChunk.length > 0) {
            chunks.push({
                start: currentStart,
                end: text.length,
                type: 'technical_step'
            });
        }

        return chunks.length > 0 ? chunks : this.createSimpleChunks(text, chunkSize);
    }

    private generateAcademicChunkingStrategy(
        text: string,
        chunkSize: number
    ): Array<{ start: number; end: number; type: string }> {
        // Look for academic sections: Abstract, Introduction, Methodology, etc.
        const sectionPattern = /^(abstract|introduction|methodology|results|conclusion|t√≥m t·∫Øt|gi·ªõi thi·ªáu|ph∆∞∆°ng ph√°p|k·∫øt qu·∫£|k·∫øt lu·∫≠n)/gmi;

        return this.createSimpleChunks(text, chunkSize).map(chunk => ({
            ...chunk,
            type: 'academic_section'
        }));
    }

    private generateSemanticChunkingStrategy(
        text: string,
        chunkSize: number,
        intelligence: DocumentIntelligence
    ): Array<{ start: number; end: number; type: string }> {
        // Use boundaries recommended by intelligence analysis
        const boundaries = intelligence.chunkingStrategy.boundaries;

        if (boundaries.includes('section')) {
            return this.createSimpleChunks(text, chunkSize).map(chunk => ({
                ...chunk,
                type: 'semantic_section'
            }));
        } else {
            return this.createSimpleChunks(text, chunkSize).map(chunk => ({
                ...chunk,
                type: 'semantic_paragraph'
            }));
        }
    }

    private splitByClauses(text: string, maxSize: number, offset: number): Array<{ start: number; end: number; type: string }> {
        const clauses = text.split(/(?=\n\d+\.)/);
        const chunks = [];

        let currentPos = offset;
        for (const clause of clauses) {
            if (clause.trim()) {
                chunks.push({
                    start: currentPos,
                    end: currentPos + clause.length,
                    type: 'legal_clause'
                });
                currentPos += clause.length;
            }
        }

        return chunks;
    }

    private async applyChunkingStrategy(
        text: string,
        strategy: Array<{ start: number; end: number; type: string }>,
        overlap: number
    ): Promise<Array<{ id: string; content: string; startIndex: number; endIndex: number; metadata: { chunkSize: number } }>> {
        return strategy.map((chunk, index) => ({
            id: `chunk_${index + 1}`,
            content: text.substring(chunk.start, chunk.end),
            startIndex: chunk.start,
            endIndex: chunk.end,
            metadata: {
                chunkSize: chunk.end - chunk.start
            }
        }));
    }

    private async enhanceChunksWithMetadata(
        chunks: Array<{ id: string; content: string; startIndex: number; endIndex: number; metadata: { chunkSize: number } }>,
        language: string
    ): Promise<Array<{ id: string; content: string; startIndex: number; endIndex: number; metadata: any }>> {
        const enhancedChunks = [];

        for (const chunk of chunks) {
            // Generate keywords and summary for each chunk
            const keywords = await this.extractKeywords({ text: chunk.content, maxKeywords: 5, language });
            const summary = await this.generateSummary({ text: chunk.content, maxLength: 100, language });

            enhancedChunks.push({
                ...chunk,
                metadata: {
                    ...chunk.metadata,
                    keywords: keywords.keywords.map(k => k.keyword),
                    summary: summary.summary,
                    structure: 'paragraph',
                    importance: this.determineImportance(chunk.content, keywords.keywords)
                }
            });
        }

        return enhancedChunks;
    } private parseStructureResponse(response: string, text: string, language: string): DocumentStructure {
        try {
            const parsed = typeof response === 'string' ? JSON.parse(response) : response;
            return {
                language,
                documentType: 'general',
                sections: parsed.sections || [],
                metadata: {
                    totalSections: parsed.sections?.length || 0,
                    estimatedReadingTime: Math.ceil(text.length / 1000),
                    complexity: text.length > 5000 ? 'complex' : text.length > 2000 ? 'medium' : 'simple'
                }
            };
        } catch (error) {
            console.error('‚ùå Failed to parse structure response:', error);
            return {
                language,
                documentType: 'general',
                sections: [],
                metadata: {
                    totalSections: 0,
                    estimatedReadingTime: Math.ceil(text.length / 1000),
                    complexity: 'simple'
                }
            };
        }
    } private async parseKeywordsResponse(response: string, language: string): Promise<KeywordsResult> {
        try {
            // Clean response and extract JSON
            let jsonText = response.trim();

            // Remove markdown code blocks
            const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)\s*```/);
            if (jsonMatch) {
                jsonText = jsonMatch[1].trim();
            }

            // Try to extract JSON object
            const jsonStart = jsonText.indexOf('{');
            const jsonEnd = jsonText.lastIndexOf('}');
            if (jsonStart >= 0 && jsonEnd > jsonStart) {
                jsonText = jsonText.substring(jsonStart, jsonEnd + 1);
            }

            const parsed = JSON.parse(jsonText);
            return {
                keywords: parsed.keywords || [],
                totalKeywords: parsed.keywords?.length || 0,
                language
            };
        } catch (error) {
            console.error('‚ùå Failed to parse keywords response:', error);
            console.error('Raw response:', response);

            // Fallback: try to extract keywords manually from response
            const keywordLines = response.split('\n').filter(line =>
                line.includes('keyword') || line.includes('t·ª´ kh√≥a') || line.match(/^\d+\./)
            );

            const fallbackKeywords = keywordLines.slice(0, 10).map((line, index) => ({
                keyword: line.replace(/^\d+\.|\*|\-|keyword:?/gi, '').trim(),
                relevance: 0.8 - (index * 0.05),
                frequency: 1
            }));

            return {
                keywords: fallbackKeywords,
                totalKeywords: fallbackKeywords.length,
                language
            };
        }
    }

    private determineImportance(content: string, keywords: Array<{ keyword: string; relevance: number }>): 'high' | 'medium' | 'low' {
        const highImportanceIndicators = ['quan tr·ªçng', 'c·∫ßn thi·∫øt', 'b·∫Øt bu·ªôc', 'nghi√™m c·∫•m', 'y√™u c·∫ßu'];
        const contentLower = content.toLowerCase();

        const hasHighIndicators = highImportanceIndicators.some(indicator => contentLower.includes(indicator));
        const avgRelevance = keywords.reduce((sum, k) => sum + k.relevance, 0) / keywords.length;

        if (hasHighIndicators || avgRelevance > 0.8) return 'high';
        if (avgRelevance > 0.5) return 'medium';
        return 'low';
    }

    /**
     * Analyze document to determine optimal chunking strategy based on content type and structure
     */
    private async analyzeDocumentIntelligence(
        params: z.infer<typeof documentIntelligenceSchema>
    ): Promise<DocumentIntelligence> {
        const { text, maxChunkSize = 1000, language = 'auto' } = params;

        const detectedLanguage = language === 'auto' ? await this.detectLanguage(text) : language;

        const prompt = this.buildDocumentIntelligencePrompt(text, maxChunkSize, detectedLanguage);
        const aiResponse = await this.callTogetherAI(prompt, this.SYSTEM_PROMPTS.DOCUMENT_INTELLIGENCE);

        return this.parseDocumentIntelligenceResponse(aiResponse, text, detectedLanguage);
    }

    private buildDocumentIntelligencePrompt(text: string, maxChunkSize: number, language: string): string {
        const langInstruction = language === 'vi'
            ? `Ph√¢n t√≠ch vƒÉn b·∫£n ti·∫øng Vi·ªát ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i t√†i li·ªáu v√† chi·∫øn l∆∞·ª£c chia nh·ªè t·ªëi ∆∞u`
            : `Analyze this document to determine document type and optimal chunking strategy`;

        return `${langInstruction}

Analyze this document sample (first 2000 characters):
${text.substring(0, 2000)}${text.length > 2000 ? '\n...[truncated]' : ''}

Document length: ${text.length} characters
Target max chunk size: ${maxChunkSize} characters

IMPORTANT: Return ONLY a valid JSON object with this exact structure:
{
  "documentType": "legal|book|technical|news|academic|contract|financial|manual|unknown",
  "confidence": 0.95,
  "structure": {
    "hasChapters": true,
    "hasSections": true,
    "hasNumberedItems": true,
    "hasHeaders": true,
    "averageParagraphLength": 150,
    "totalParagraphs": 10
  },
  "chunkingStrategy": {
    "recommendedChunkSize": 800,
    "chunkingMethod": "semantic|structural|hybrid",
    "preserveStructure": true,
    "boundaries": ["sentence", "paragraph", "section"]
  },
  "contentAnalysis": {
    "density": "medium",
    "technicalTerms": 15,
    "legalTerms": 8,
    "complexity": "medium"
  },
  "language": "${language}",
  "estimatedProcessingTime": 5000
}`;
    }

    private parseDocumentIntelligenceResponse(response: string, text: string, language: string): DocumentIntelligence {
        try {
            // Clean response and extract JSON
            let jsonText = response.trim();

            // Remove markdown code blocks
            const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)\s*```/);
            if (jsonMatch) {
                jsonText = jsonMatch[1].trim();
            }

            // Try to extract JSON object
            const jsonStart = jsonText.indexOf('{');
            const jsonEnd = jsonText.lastIndexOf('}');
            if (jsonStart >= 0 && jsonEnd > jsonStart) {
                jsonText = jsonText.substring(jsonStart, jsonEnd + 1);
            }

            const parsed = JSON.parse(jsonText);

            // Validate and return with defaults
            return {
                documentType: parsed.documentType || 'unknown',
                confidence: parsed.confidence || 0.5,
                structure: {
                    hasChapters: parsed.structure?.hasChapters || false,
                    hasSections: parsed.structure?.hasSections || false,
                    hasNumberedItems: parsed.structure?.hasNumberedItems || false,
                    hasHeaders: parsed.structure?.hasHeaders || false,
                    averageParagraphLength: parsed.structure?.averageParagraphLength || 100,
                    totalParagraphs: parsed.structure?.totalParagraphs || Math.ceil(text.length / 500)
                },
                chunkingStrategy: {
                    recommendedChunkSize: parsed.chunkingStrategy?.recommendedChunkSize || 1000,
                    chunkingMethod: parsed.chunkingStrategy?.chunkingMethod || 'hybrid',
                    preserveStructure: parsed.chunkingStrategy?.preserveStructure !== false,
                    boundaries: parsed.chunkingStrategy?.boundaries || ['paragraph']
                },
                contentAnalysis: {
                    density: parsed.contentAnalysis?.density || 'medium',
                    technicalTerms: parsed.contentAnalysis?.technicalTerms || 0,
                    legalTerms: parsed.contentAnalysis?.legalTerms || 0,
                    complexity: parsed.contentAnalysis?.complexity || 'medium'
                },
                language,
                estimatedProcessingTime: parsed.estimatedProcessingTime || 5000
            };
        } catch (error) {
            console.error('‚ùå Failed to parse document intelligence response:', error);
            console.error('Raw response:', response);

            // Fallback: basic analysis
            return this.createFallbackDocumentIntelligence(text, language);
        }
    }

    private createFallbackDocumentIntelligence(text: string, language: string): DocumentIntelligence {
        // Basic pattern-based analysis
        const hasChapters = /ch∆∞∆°ng|chapter|ph·∫ßn/i.test(text);
        const hasLegalTerms = /ƒëi·ªÅu|kho·∫£n|ngh·ªã ƒë·ªãnh|lu·∫≠t|quy ƒë·ªãnh/i.test(text);
        const hasTechnicalTerms = /h∆∞·ªõng d·∫´n|c√°ch th·ª©c|b∆∞·ªõc|th·ª±c hi·ªán/i.test(text);

        let documentType: DocumentIntelligence['documentType'] = 'unknown';
        if (hasLegalTerms) documentType = 'legal';
        else if (hasChapters) documentType = 'book';
        else if (hasTechnicalTerms) documentType = 'technical';

        const paragraphs = text.split('\n\n').length;
        const avgParagraphLength = paragraphs > 0 ? Math.round(text.length / paragraphs) : 100;

        return {
            documentType,
            confidence: 0.6,
            structure: {
                hasChapters,
                hasSections: /m·ª•c|section|ph·∫ßn/i.test(text),
                hasNumberedItems: /^\d+\./m.test(text),
                hasHeaders: /#|^[A-Z][^a-z]*$/m.test(text),
                averageParagraphLength: avgParagraphLength,
                totalParagraphs: paragraphs
            },
            chunkingStrategy: {
                recommendedChunkSize: documentType === 'legal' ? 500 : documentType === 'book' ? 1500 : 1000,
                chunkingMethod: documentType === 'legal' ? 'structural' : 'hybrid',
                preserveStructure: true,
                boundaries: documentType === 'legal' ? ['section'] : ['paragraph']
            },
            contentAnalysis: {
                density: text.length > 10000 ? 'high' : text.length > 3000 ? 'medium' : 'low',
                technicalTerms: (text.match(/h∆∞·ªõng d·∫´n|c√°ch th·ª©c|th·ª±c hi·ªán|quy tr√¨nh/gi) || []).length,
                legalTerms: (text.match(/ƒëi·ªÅu|kho·∫£n|ngh·ªã ƒë·ªãnh|lu·∫≠t|quy ƒë·ªãnh/gi) || []).length,
                complexity: text.length > 10000 ? 'complex' : 'medium'
            },
            language,
            estimatedProcessingTime: Math.min(Math.max(text.length / 10, 1000), 30000)
        };
    }

    private createSimpleChunks(text: string, chunkSize: number): Array<{ start: number; end: number; type: string }> {
        const chunks = [];
        let start = 0;

        while (start < text.length) {
            let end = Math.min(start + chunkSize, text.length);

            // Try to break at sentence boundary if not at end of text
            if (end < text.length) {
                // Look for sentence endings (., !, ?) within last 100 chars
                const searchStart = Math.max(start, end - 100);
                const chunk = text.substring(searchStart, end);
                const sentenceEnd = chunk.lastIndexOf('.');
                const exclamationEnd = chunk.lastIndexOf('!');
                const questionEnd = chunk.lastIndexOf('?');

                const lastSentenceEnd = Math.max(sentenceEnd, exclamationEnd, questionEnd);

                if (lastSentenceEnd > 0) {
                    end = searchStart + lastSentenceEnd + 1;
                } else {
                    // Fall back to word boundary
                    const wordBoundary = text.lastIndexOf(' ', end);
                    if (wordBoundary > start + chunkSize * 0.7) {
                        end = wordBoundary;
                    }
                }
            }

            chunks.push({
                start,
                end,
                type: 'paragraph'
            });

            start = end;
        } return chunks;
    }
}
