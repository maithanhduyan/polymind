# Generated by Copilot

"""
VAS Vietnam ChromaDB Ingestion
ThÃªm táº¥t cáº£ chunks tá»« vas_vietnam_chunked.json vÃ o ChromaDB thÃ´ng qua MCP tools
"""

import json
import requests
import time
from pathlib import Path


def load_vas_chunks():
    """Load chunks tá»« file JSON"""
    json_file = Path(__file__).parent / "vas_vietnam_chunked.json"

    with open(json_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    return data


def add_chunks_via_mcp_tools(chunks_data):
    """ThÃªm chunks vÃ o ChromaDB qua MCP tools HTTP API"""

    chunks = chunks_data["chunks"]
    mcp_endpoint = "http://localhost:3000"

    print(f"ğŸ”„ Äang thÃªm {len(chunks)} chunks vÃ o ChromaDB...")

    batch_size = 5  # Giáº£m batch size Ä‘á»ƒ trÃ¡nh lá»—i
    success_count = 0
    failed_batches = []

    for i in range(0, len(chunks), batch_size):
        batch = chunks[i : i + batch_size]

        # Chuáº©n bá»‹ dá»¯ liá»‡u cho batch
        documents = []
        metadatas = []
        ids = []

        for chunk in batch:
            # Táº¡o document text
            doc_text = f"{chunk['title']}\n\n{chunk['content']}"
            documents.append(doc_text)

            # Táº¡o metadata (lÃ m sáº¡ch cÃ¡c giÃ¡ trá»‹ None)
            metadata = {
                "id": chunk["id"],
                "type": chunk["type"],
                "title": chunk["title"],
                "keywords": ",".join(chunk["metadata"].get("keywords", [])),
                "importance": chunk["metadata"].get("importance", "medium"),
                "section_type": chunk["metadata"].get("section_type", "unknown"),
                "legal_status": chunk["metadata"].get("legal_status", "active"),
            }

            # ThÃªm standard_number náº¿u cÃ³
            if chunk.get("standard_number") is not None:
                metadata["standard_number"] = chunk["standard_number"]

            # ThÃªm section_title náº¿u cÃ³
            if chunk["metadata"].get("section_title"):
                metadata["section_title"] = chunk["metadata"]["section_title"]

            metadatas.append(metadata)
            ids.append(chunk["id"])

        # Gá»i MCP tool Ä‘á»ƒ thÃªm vÃ o ChromaDB
        try:
            # URL Ä‘Ãºng cho MCP tools
            url = f"{mcp_endpoint}/mcp/tools/mcp_tools-mcp_chromadb_chromadb_add_documents"

            payload = {
                "collectionName": "vas_vietnam_2025",
                "documents": documents,
                "metadatas": metadatas,
                "ids": ids,
            }

            response = requests.post(url, json=payload, timeout=30)

            if response.status_code == 200:
                result = response.json()
                if result.get("success"):
                    success_count += len(batch)
                    print(
                        f"âœ… Batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}: ThÃªm {len(batch)} chunks"
                    )
                else:
                    print(
                        f"âŒ Batch {i//batch_size + 1}: Response khÃ´ng thÃ nh cÃ´ng - {result}"
                    )
                    failed_batches.append(i // batch_size + 1)
            else:
                print(
                    f"âŒ Batch {i//batch_size + 1}: HTTP {response.status_code} - {response.text}"
                )
                failed_batches.append(i // batch_size + 1)

        except Exception as e:
            print(f"âŒ Batch {i//batch_size + 1}: Exception - {e}")
            failed_batches.append(i // batch_size + 1)

        # Nghá»‰ má»™t chÃºt giá»¯a cÃ¡c batch
        time.sleep(0.5)

    print(f"\nğŸ‰ HoÃ n thÃ nh! ÄÃ£ thÃªm {success_count}/{len(chunks)} chunks")
    if failed_batches:
        print(f"âŒ CÃ¡c batch tháº¥t báº¡i: {failed_batches}")

    return success_count


def test_semantic_search_comprehensive():
    """Test tÃ¬m kiáº¿m ngá»¯ nghÄ©a toÃ n diá»‡n"""
    print("\nğŸ” Testing comprehensive semantic search...")

    mcp_endpoint = "http://localhost:3000"

    test_queries = [
        "chuáº©n má»±c chung vá» káº¿ toÃ¡n",
        "hÃ ng tá»“n kho Ä‘Ã¡nh giÃ¡ vÃ  ghi nháº­n",
        "tÃ i sáº£n cá»‘ Ä‘á»‹nh há»¯u hÃ¬nh kháº¥u hao",
        "tÃ i sáº£n cá»‘ Ä‘á»‹nh vÃ´ hÃ¬nh",
        "báº¥t Ä‘á»™ng sáº£n Ä‘áº§u tÆ°",
        "thuÃª tÃ i sáº£n hoáº¡t Ä‘á»™ng",
        "bÃ¡o cÃ¡o tÃ i chÃ­nh há»£p nháº¥t",
        "thuáº¿ thu nháº­p doanh nghiá»‡p",
        "doanh thu vÃ  thu nháº­p khÃ¡c",
        "chi phÃ­ Ä‘i vay vá»‘n hÃ³a",
    ]

    for query in test_queries:
        try:
            url = f"{mcp_endpoint}/mcp/tools/mcp_tools-mcp_chromadb_chromadb_query"
            payload = {
                "collectionName": "vas_vietnam_2025",
                "queryTexts": [query],
                "nResults": 2,
                "includeMetadata": True,
                "includeDistances": True,
            }

            response = requests.post(url, json=payload, timeout=10)

            if response.status_code == 200:
                results = response.json()
                print(f"\nğŸ“‹ Query: '{query}'")

                if (
                    results.get("results", {}).get("documents")
                    and results["results"]["documents"][0]
                ):
                    docs = results["results"]["documents"][0]
                    metadatas = results["results"]["metadatas"][0]
                    distances = results["results"]["distances"][0]

                    for j, doc in enumerate(docs):
                        metadata = metadatas[j]
                        distance = distances[j]
                        title = metadata.get("title", "Unknown")
                        standard_num = metadata.get("standard_number", "N/A")
                        print(f"  {j+1}. {title}")
                        print(
                            f"     Chuáº©n má»±c: {standard_num}, Distance: {distance:.3f}"
                        )
                        print(f"     Preview: {doc[:100]}...")
                else:
                    print("     KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£")
            else:
                print(f"âŒ Query failed: {response.status_code} - {response.text}")

        except Exception as e:
            print(f"âŒ Error testing query '{query}': {e}")


def get_collection_stats():
    """Láº¥y thá»‘ng kÃª collection"""
    print("\nğŸ“Š Collection Statistics...")

    mcp_endpoint = "http://localhost:3000"

    try:
        url = f"{mcp_endpoint}/mcp/tools/mcp_tools-mcp_chromadb_chromadb_get_collection"
        payload = {"name": "vas_vietnam_2025"}

        response = requests.post(url, json=payload, timeout=10)

        if response.status_code == 200:
            result = response.json()
            print(f"âœ… Collection: {result.get('name', 'Unknown')}")
            print(f"ğŸ“„ Document count: {result.get('count', 0)}")
            print(f"ğŸ†” Collection ID: {result.get('id', 'Unknown')}")

            metadata = result.get("metadata", {})
            print(f"ğŸ“… Created: {metadata.get('created_at', 'Unknown')}")
            print(f"ğŸŒ Language: {metadata.get('language', 'Unknown')}")
            print(f"ğŸ“– Document type: {metadata.get('document_type', 'Unknown')}")
        else:
            print(f"âŒ Failed to get collection stats: {response.text}")

    except Exception as e:
        print(f"âŒ Error getting collection stats: {e}")


def main():
    """HÃ m main"""
    print("ğŸš€ VAS Vietnam ChromaDB Ingestion via MCP Tools")
    print("=" * 60)

    try:
        # Load chunks
        chunks_data = load_vas_chunks()
        total_chunks = len(chunks_data["chunks"])
        print(f"ğŸ“– Loaded {total_chunks} chunks from JSON file")

        # Show some stats
        metadata = chunks_data["metadata"]
        print(f"ğŸ“‹ Document: {metadata['document_title']}")
        print(f"ğŸŒ Language: {metadata['language']}")
        print(f"ğŸ“… Created: {metadata['created_at']}")

        # Add chunks to ChromaDB
        success_count = add_chunks_via_mcp_tools(chunks_data)

        if success_count > 0:
            # Get collection stats
            get_collection_stats()

            # Test semantic search
            test_semantic_search_comprehensive()

            print("\n" + "=" * 60)
            print("âœ… VAS Vietnam ingestion completed!")
            print(f"ğŸ“Š Successfully ingested: {success_count}/{total_chunks} chunks")
            print(f"ğŸ—„ï¸  Collection: vas_vietnam_2025")
            print("ğŸ” Ready for semantic search on Vietnamese Accounting Standards")

        else:
            print("âŒ No chunks were successfully added to ChromaDB")

    except Exception as e:
        print(f"âŒ Error: {e}")


if __name__ == "__main__":
    main()
